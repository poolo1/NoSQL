{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NoSQL\n",
    "### Exercices Chapitre II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On importe les packagex nécessaire à l'exercice.\n",
    "import tqdm\n",
    "import json\n",
    "import pickle\n",
    "import lxml.etree\n",
    "import lorem\n",
    "from PIL import Image\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Put a Lorem Ipsum of 3 paragraphs in a txt file using python, each paragraph delimited by 2 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ipsum eius non modi tempora non quiquia. Aliquam neque dolorem adipisci. Numquam velit eius ut. Porro numquam tempora numquam dolorem tempora. Sit dolorem est adipisci. Adipisci aliquam etincidunt porro. Adipisci ut sed non amet voluptatem. Dolore numquam amet magnam numquam.\n",
      "\n",
      "Quaerat eius adipisci amet ut non. Quaerat numquam tempora quisquam adipisci porro tempora. Quaerat porro aliquam numquam. Magnam dolore ipsum sed. Numquam non adipisci dolorem magnam. Velit eius voluptatem aliquam modi ipsum. Eius ut aliquam dolorem sed magnam sed tempora.\n",
      "\n",
      "Ut dolor numquam quiquia consectetur aliquam ipsum. Aliquam eius numquam neque sed neque. Eius ipsum dolorem amet. Sed est velit consectetur quiquia. Sed sit amet porro. Etincidunt porro quisquam porro sit porro. Amet tempora eius amet amet tempora eius. Quiquia eius numquam sed ipsum. Dolorem amet quiquia adipisci.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#On enlève fait un saut de deux lignes entre chaque ligne extraite de lorem_ipsum\n",
    "with open('lorem_ipsum.txt', 'w') as file:\n",
    "    for i in range(4):\n",
    "        paragraph = lorem.paragraph()\n",
    "        file.write(paragraph)\n",
    "        file.write('\\n\\n')\n",
    "\n",
    "with open('lorem_ipsum.txt', 'r') as file:\n",
    "    paragraphs = file.readlines()\n",
    "\n",
    "paragraphs.pop(0)\n",
    "\n",
    "with open('lorem_ipsum.txt', 'w') as file:\n",
    "    for paragraph in paragraphs:\n",
    "        file.write(paragraph)\n",
    "        print(paragraph.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Update the txt file by removiing the first paragraph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Quaerat eius adipisci amet ut non. Quaerat numquam tempora quisquam adipisci porro tempora. Quaerat porro aliquam numquam. Magnam dolore ipsum sed. Numquam non adipisci dolorem magnam. Velit eius voluptatem aliquam modi ipsum. Eius ut aliquam dolorem sed magnam sed tempora.\n",
      "\n",
      "Ut dolor numquam quiquia consectetur aliquam ipsum. Aliquam eius numquam neque sed neque. Eius ipsum dolorem amet. Sed est velit consectetur quiquia. Sed sit amet porro. Etincidunt porro quisquam porro sit porro. Amet tempora eius amet amet tempora eius. Quiquia eius numquam sed ipsum. Dolorem amet quiquia adipisci.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('lorem_ipsum.txt', 'r') as file:\n",
    "    paragraphs = file.readlines()\n",
    "\n",
    "paragraphs.pop(1)\n",
    "\n",
    "with open('lorem_ipsum.txt', 'w') as file:\n",
    "    for paragraph in paragraphs:\n",
    "        file.write(paragraph)\n",
    "\n",
    "with open('lorem_ipsum.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "print(content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a dictionary from the paper locun et al. and goodfollow et al. with authors, title, affiliations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LeCun et al.': {'authors': ['Yann LeCun', 'Yoshua Bengio', 'Geoffrey Hinton'], 'title': 'Deep Learning', 'affiliations': ['Facebook AI Research, New York University', 'Department of Computer Science and Operations Research Université de Montréal', 'Google, Department of Computer Science, University of Toronto']}, 'Goodfellow et al.': {'authors': ['Ian Goodfellow', 'Jean Pouget-Abadie', 'Mehdi Mirza', 'Bing Xu', 'David Warde-Farley', 'Sherjil Ozair', 'Yoshua Bengio', 'Aaron Courville'], 'title': 'Deep Learning', 'affiliations': ['University of Montreal', 'Ecole Polytechnique', 'Université de Montréal', 'Université de Montréal', 'Université de Montréal', 'Indian Institute of Technology Delhi', 'Université de Montréal', 'CIFAR Senior Fellow']}}\n"
     ]
    }
   ],
   "source": [
    "lecun_paper = {\n",
    "    \"authors\": [\"Yann LeCun\", \"Yoshua Bengio\", \"Geoffrey Hinton\"],\n",
    "    \"title\": \"Deep Learning\",\n",
    "    \"affiliations\": [\"Facebook AI Research, New York University\", \"Department of Computer Science and Operations Research Université de Montréal\", \"Google, Department of Computer Science, University of Toronto\"]\n",
    "    }\n",
    "    \n",
    "\n",
    "goodfellow_paper = {\n",
    "    \"authors\": [\"Ian Goodfellow\", \"Jean Pouget-Abadie\", \"Mehdi Mirza\", \"Bing Xu\", \"David Warde-Farley\", \"Sherjil Ozair\", \"Yoshua Bengio\", \"Aaron Courville\"],\n",
    "        \"title\": \"Deep Learning\",\n",
    "        \"affiliations\": [\"University of Montreal\", \"Ecole Polytechnique\", \"Université de Montréal\", \"Université de Montréal\", \"Université de Montréal\", \"Indian Institute of Technology Delhi\", \"Université de Montréal\",  \"CIFAR Senior Fellow\",]\n",
    "    }\n",
    "\n",
    "\n",
    "papers2 = {\n",
    "    \"LeCun et al.\": lecun_paper,\n",
    "    \"Goodfellow et al.\": goodfellow_paper\n",
    "}\n",
    "print(papers2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Save the previuosly created dictionary in the JSON format and load it back.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LeCun et al.': {'authors': ['Yann LeCun', 'Yoshua Bengio', 'Geoffrey Hinton'], 'title': 'Deep Learning', 'affiliations': ['Facebook AI Research, New York University', 'Department of Computer Science and Operations Research Université de Montréal', 'Google, Department of Computer Science, University of Toronto']}, 'Goodfellow et al.': {'authors': ['Ian Goodfellow', 'Jean Pouget-Abadie', 'Mehdi Mirza', 'Bing Xu', 'David Warde-Farley', 'Sherjil Ozair', 'Yoshua Bengio', 'Aaron Courville'], 'title': 'Deep Learning', 'affiliations': ['University of Montreal', 'Ecole Polytechnique', 'Université de Montréal', 'Université de Montréal', 'Université de Montréal', 'Indian Institute of Technology Delhi', 'Université de Montréal', 'CIFAR Senior Fellow']}}\n"
     ]
    }
   ],
   "source": [
    "with open('paper2.json', 'w') as file:\n",
    "    json.dump(papers2, file)\n",
    "with open('paper2.json', 'r') as file:\n",
    "    loaded_papers = json.load(file)\n",
    "print(loaded_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Save the previuosly created dictionary in the pickle format. Try to open manually, is it human readable ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x80\\x04\\x95\\x86\\x02\\x00\\x00\\x00\\x00\\x00\\x00}\\x94(\\x8c\\x0cLeCun et al.\\x94}\\x94(\\x8c\\x07authors\\x94]\\x94(\\x8c\\nYann LeCun\\x94\\x8c\\rYoshua Bengio\\x94\\x8c\\x0fGeoffrey Hinton\\x94e\\x8c\\x05title\\x94\\x8c\\rDeep Learning\\x94\\x8c\\x0caffiliations\\x94]\\x94(\\x8c)Facebook AI Research, New York University\\x94\\x8cODepartment of Computer Science and Operations Research Universit\\xc3\\xa9 de Montr\\xc3\\xa9al\\x94\\x8c=Google, Department of Computer Science, University of Toronto\\x94eu\\x8c\\x11Goodfellow et al.\\x94}\\x94(h\\x03]\\x94(\\x8c\\x0eIan Goodfellow\\x94\\x8c\\x12Jean Pouget-Abadie\\x94\\x8c\\x0bMehdi Mirza\\x94\\x8c\\x07Bing Xu\\x94\\x8c\\x12David Warde-Farley\\x94\\x8c\\rSherjil Ozair\\x94\\x8c\\rYoshua Bengio\\x94\\x8c\\x0fAaron Courville\\x94eh\\x08\\x8c\\rDeep Learning\\x94h\\n]\\x94(\\x8c\\x16University of Montreal\\x94\\x8c\\x13Ecole Polytechnique\\x94\\x8c\\x18Universit\\xc3\\xa9 de Montr\\xc3\\xa9al\\x94h\\x1eh\\x1e\\x8c$Indian Institute of Technology Delhi\\x94h\\x1e\\x8c\\x13CIFAR Senior Fellow\\x94euu.'\n"
     ]
    }
   ],
   "source": [
    "with open('papers.pickle', 'wb') as file:\n",
    "    pickle.dump(papers2, file)\n",
    "with open('papers.pickle', 'rb') as file:\n",
    "    contents = file.read()\n",
    "\n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce n'est **pas** lisible pour l'humain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Parse the xlm_flie2 in the same way as in the lecture. Put info in a dict and save it in a JSON file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': ['2015-09-01'], 'hour': ['08:30'], 'to': ['Tove'], 'from': ['Jani'], 'body': [\"Don't forget me this weekend!\"]}\n"
     ]
    }
   ],
   "source": [
    "xml_file2 = \"/Users/polo11/Downloads/xml_file2.nxml\"\n",
    "root = lxml.etree.parse(xml_file2)\n",
    "\n",
    "# On stock les différentes infos\n",
    "date = root.xpath(\"//date//text()\")\n",
    "hour = root.xpath(\"//hour//text()\")\n",
    "to = root.xpath(\"//to//text()\")\n",
    "from_ = root.xpath(\"//from//text()\")\n",
    "body = root.xpath(\"//body//text()\")\n",
    "\n",
    "# On créé un dict avec les infos\n",
    "info_dict = {\n",
    "    \"date\": date,\n",
    "    \"hour\": hour,\n",
    "    \"to\": to,\n",
    "    \"from\": from_,\n",
    "    \"body\": body\n",
    "}\n",
    "#ON enregistre dans un fichier .JSON\n",
    "with open(\"xml_data.json\", \"w\") as outfile:\n",
    "    json.dump(info_dict, outfile)\n",
    "\n",
    "with open('xml_data.json', 'r') as file:\n",
    "    contents = json.load(file)\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Downloads an image of your choice and save it either jpg or png.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(requests.get(\"https://static.lpnt.fr/images/2021/11/17/22442307-22442297-g-jpg_8375297_1000x667.jpg\", stream=True).raw)\n",
    "im.save(\"/Users/polo11/Downloads/singe_ouille.png\", \"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. From the data/Chap2/data_world.json file, create a set of publisher type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'org:Organization'}\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON file\n",
    "with open('data_world.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "publisher_types = {entry['publisher']['@type'] for entry in data if isinstance(entry, dict) and 'publisher' in entry and isinstance(entry['publisher'], dict) and '@type' in entry['publisher']}\n",
    "print(publisher_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. From the data/Chap2/data_world.json file, delete the key of your choice and save the new dict as data_world_cleaned.json.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/polo11/Downloads/data_world.json', 'r', encoding='utf-8') as fp:\n",
    "    docs = json.load(fp)\n",
    "\n",
    "[doc.pop(\"accessLevel\") for doc in docs if doc[\"publisher\"][\"@type\"] == \"org:Organization\"]\n",
    "\n",
    "with open('/Users/polo11/Downloads/data_world_cleaned.json', 'w', encoding='utf-8') as fp:\n",
    "    json.dump(docs, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. From the data/Chap2/data_world.json file, create the co-occurence matrix between \"accessLevel\" and \"accrualPeriodicity\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accrualPeriodicity  R/P1D  R/P1M  R/P3M  R/PT1S  irregular\n",
      "accessLevel                                               \n",
      "public                  5      3      1       1       4961\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/polo11/Downloads/data_world.json', 'r', encoding='utf-8') as fp:\n",
    "    docs = json.load(fp)\n",
    "\n",
    "df = pd.DataFrame(docs)\n",
    "matrix = pd.crosstab(df['accessLevel'], df['accrualPeriodicity'])\n",
    "print(matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
